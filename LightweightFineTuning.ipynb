{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663c99a4-fea3-4154-b12c-d158c179a8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pip install peft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08ac24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b170d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02459acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Evaluation: {'accuracy': 0.55}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b052edd8d34fc4be743dc3380d08d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c11a5fc99a2d4766b59ef1303c0d7363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6969479322433472, 'eval_accuracy': 0.45, 'eval_runtime': 2.8767, 'eval_samples_per_second': 6.952, 'eval_steps_per_second': 1.738, 'epoch': 1.0}\n",
      "{'train_runtime': 34.922, 'train_samples_per_second': 2.864, 'train_steps_per_second': 0.716, 'train_loss': 0.6963013458251953, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT Model Evaluation: {'accuracy': 0.45}\n"
     ]
    }
   ],
   "source": [
    "# Loading the model\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Evaluating the model\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"stanfordnlp/imdb\"\n",
    "imdb = load_dataset(dataset_name)\n",
    "\n",
    "# Use a smaller subset of the dataset\n",
    "small_train_dataset = imdb[\"train\"].shuffle(seed=42).select(range(100))  # Further reduce to 100 samples for training\n",
    "small_test_dataset = imdb[\"test\"].shuffle(seed=42).select(range(20))    # Further reduce to 20 samples for testing\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "tokenized_train_dataset = small_train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = small_test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Convert datasets to PyTorch format\n",
    "tokenized_train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Evaluate the model\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "def evaluate(model, dataset, batch_size=16):\n",
    "    model.eval()\n",
    "    eval_dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in eval_dataloader:\n",
    "        inputs = {\n",
    "            'input_ids': batch['input_ids'].squeeze(1),\n",
    "            'attention_mask': batch['attention_mask'].squeeze(1)\n",
    "        }\n",
    "        labels = batch[\"label\"].to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        all_logits.append(logits.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    all_logits = np.concatenate(all_logits, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    return compute_metrics((all_logits, all_labels))\n",
    "\n",
    "# Evaluate the base model before fine-tuning\n",
    "base_eval_results = evaluate(model, tokenized_test_dataset)\n",
    "print(f\"Initial Evaluation: {base_eval_results}\")\n",
    "\n",
    "# Performing Parameter-Efficient Fine-Tuning\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "# Create a custom PEFT configuration\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_lin\", \"v_lin\"]\n",
    ")\n",
    "\n",
    "# Creating a PEFT model\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Training the model\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,  # Reduce batch size\n",
    "    per_device_eval_batch_size=4,   # Reduce batch size\n",
    "    num_train_epochs=1,             # Reduce the number of epochs\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Saving the trained model\n",
    "peft_model.save_pretrained(\"./peft_model\")\n",
    "\n",
    "# Loading the trained PEFT model\n",
    "peft_model = AutoModelForSequenceClassification.from_pretrained(\"./peft_model\")\n",
    "\n",
    "# Re-evaluate the PEFT model\n",
    "peft_eval_results = evaluate(peft_model, tokenized_test_dataset)\n",
    "print(f\"PEFT Model Evaluation: {peft_eval_results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfa674e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
